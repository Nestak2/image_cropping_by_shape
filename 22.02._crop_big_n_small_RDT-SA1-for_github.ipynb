{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for image cropping and perspective transform\n",
    "The code looks for 2 rectangular shapes in the image (see figure below), cropes them and eventually crops from them a specified area of interest (see below). Ratio of sizes of the 2 devices need to be provided. Can be easy adjust to also work with just one device (or another number of devices).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1|2|3|4|5\n",
    "-|-|-|-|- \n",
    "<img src=\"im46.jpg\" alt=\"Drawing\" width=\"420\"/> | <img src=\"small_im46.jpg\" alt=\"Drawing\" width=\"150\"/> | <img src=\"im46 2.jpg\" alt=\"Drawing\" width=\"150\"/> | <img src=\"AOIsmall_im46.jpg\" alt=\"Drawing\" width=\"50\"/>| <img src=\"AOIim46.jpg\" alt=\"Drawing\" width=\"50\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of work of the script. In yellow frame are the 2 devices, in blue are the areas of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch.transform import four_point_transform\n",
    "#from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import imutils\n",
    "from shutil import copyfile\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User selected input values\n",
    "The first part needs user change, the second part can probably be kept with the default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### the following features need to be changed by the user to desired values\n",
    "\n",
    "# file path to the input images, that will be cropped by the script\n",
    "input_images_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/Val_AHRI_Test_images/'\n",
    "# output folder where the intermediate cropped images will be stored\n",
    "output_cropped_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/cropped/'\n",
    "# output folder where the images will be stored in which the squares could not be successfully detected\n",
    "output_uncropped_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/uncropped/'\n",
    "# output folder where eventually the defined areas of interest from the images will be stored\n",
    "output_AOI_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/AOI/'\n",
    "\n",
    "# define the coordinates of your areas of interest, as height and width ratio in respect to the 4 squares.\n",
    "# The coordinates will have the form of: \n",
    "# image_AOI = im[int(x0*height_im):int(x1*height_im), int(y0*width_im):int(y1*width_im)]\n",
    "# In this script 2 devixes are being detected, L is the large and S is the small\n",
    "\n",
    "L_top, L_bottom = 0.44, 0.65\n",
    "L_left, L_right = 0.37, 0.67\n",
    "S_top, S_bottom = 0.28, 0.63\n",
    "S_left, S_right = 0.32, 0.66\n",
    "\n",
    "####### the features below this line can be probably kept with the given default values,\n",
    "####### but the user may consider changing them if script performance is bad\n",
    "\n",
    "# what type of image files do you want the script to opearate on\n",
    "file_extensions = ['.jpg', '.png']\n",
    "\n",
    "# # the size of the kernel when threshholding is being performed\n",
    "# thresh_size = 15\n",
    "\n",
    "# the height of the image will be scaled down to an image with this height, so that working can go faster \n",
    "height_working_image=500.\n",
    "\n",
    "# defining the range of blurness in which the image will be looped for the detection of shapes\n",
    "GBlur_bottom_value=7\n",
    "GBlur_top_value=21\n",
    "GBlur_interval=2\n",
    "\n",
    "# are_ratio determines the approximate ratio (pixel-area small device)/(pixel-area large device)\n",
    "# the lower and top biundaries determine the tolerance range around the area_ratio, e.g.\n",
    "# for a object to be considered small device it has to fulfill \n",
    "# area_small>lower_area_boundary*area_ratio*area_big and area_small<top_area_boundary*area_ratio*area_big\n",
    "area_ratio=1/1.46\n",
    "lower_area_boundary, top_area_boundary = 0.7, 1.3\n",
    "\n",
    "#define the range of tolerance for the ratio of h/w for the size of the large device\n",
    "h_w_ratio_large_lower=2.6\n",
    "h_w_ratio_large_upper=3.5\n",
    "\n",
    "# the size of the blurring kernel for the function for correct orientation\n",
    "blur_size_correct_orientation = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "\t# initialzie a list of coordinates that will be ordered\n",
    "\t# such that the first entry in the list is the top-left,\n",
    "\t# the second entry is the top-right, the third is the\n",
    "\t# bottom-right, and the fourth is the bottom-left\n",
    "\trect = np.zeros((4, 2), dtype = \"float32\")\n",
    " \n",
    "\t# the top-left point will have the smallest sum, whereas\n",
    "\t# the bottom-right point will have the largest sum\n",
    "\ts = pts.sum(axis = 1)\n",
    "\trect[0] = pts[np.argmin(s)]\n",
    "\trect[2] = pts[np.argmax(s)]\n",
    " \n",
    "\t# now, compute the difference between the points, the\n",
    "\t# top-right point will have the smallest difference,\n",
    "\t# whereas the bottom-left will have the largest difference\n",
    "\tdiff = np.diff(pts, axis = 1)\n",
    "\trect[1] = pts[np.argmin(diff)]\n",
    "\trect[3] = pts[np.argmax(diff)]\n",
    " \n",
    "\t# return the ordered coordinates\n",
    "\treturn rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "\t# obtain a consistent order of the points and unpack them\n",
    "\t# individually\n",
    "\trect = order_points(pts)\n",
    "\t(tl, tr, br, bl) = rect\n",
    " \n",
    "\t# compute the width of the new image, which will be the\n",
    "\t# maximum distance between bottom-right and bottom-left\n",
    "\t# x-coordiates or the top-right and top-left x-coordinates\n",
    "\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "\tmaxWidth = max(int(widthA), int(widthB))\n",
    " \n",
    "\t# compute the height of the new image, which will be the\n",
    "\t# maximum distance between the top-right and bottom-right\n",
    "\t# y-coordinates or the top-left and bottom-left y-coordinates\n",
    "\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "\tmaxHeight = max(int(heightA), int(heightB))\n",
    " \n",
    "\t# now that we have the dimensions of the new image, construct\n",
    "\t# the set of destination points to obtain a \"birds eye view\",\n",
    "\t# (i.e. top-down view) of the image, again specifying points\n",
    "\t# in the top-left, top-right, bottom-right, and bottom-left\n",
    "\t# order\n",
    "\tdst = np.array([\n",
    "\t\t[0, 0],\n",
    "\t\t[maxWidth - 1, 0],\n",
    "\t\t[maxWidth - 1, maxHeight - 1],\n",
    "\t\t[0, maxHeight - 1]], dtype = \"float32\")\n",
    " \n",
    "\t# compute the perspective transform matrix and then apply it\n",
    "\tM = cv2.getPerspectiveTransform(rect, dst)\n",
    "\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    " \n",
    "\t# return the warped image\n",
    "\treturn warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def width_calc(quader):\n",
    "    '''Function that assumes quader is rectanlge and returns its width'''\n",
    "    quad = np.resize(quader, (4,2))\n",
    "    distances = []\n",
    "    for coo in quad[1:]:\n",
    "        d = np.sqrt((quad[0][0]-coo[0])**2 + (quad[0][1]-coo[1])**2)\n",
    "        distances.append(d)\n",
    "\n",
    "    return(min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_folder, output_folder, output_uncrop_folder, file_extensions, \n",
    "         height_working_image, area_ratio, h_w_ratio_large_upper, h_w_ratio_large_lower):\n",
    "    \"\"\"Main function, contains the loop and all the sub-functions in it.\n",
    "    DON'T forget to add an extra '/' at the end of input and output folder.\n",
    "    area_ratio is the ratio (imaged area small device)/(imaged area large device).\n",
    "    h_w_ratio_large_upper and h_w_ratio_large_lower are the range margins for the ratio h/w between the lenght and width of the imaged large device.\"\"\"\n",
    "    # create a list of all the image-paths in a given folder\n",
    "\n",
    "    file_names = [fn for fn in sorted(os.listdir(input_folder))\n",
    "                  if any(fn.endswith(ext) for ext in file_extensions)]\n",
    "    paths = [input_folder+file_name for file_name in file_names]\n",
    "    \n",
    "    flag_L = 'stay in loop'\n",
    "    flag_S = 'stay in loop'\n",
    "    i = 0\n",
    "                \n",
    "    # loop through all of the image-paths\n",
    "    for im_path in paths[:]:\n",
    "        print('loop:', i)\n",
    "        \n",
    "        if i>0 and flag_L == 'stay in loop':\n",
    "            copyfile(paths[i-1], output_uncrop_folder+'large_uncr'+path_end)\n",
    "        if i>0 and flag_S == 'stay in loop':\n",
    "            copyfile(paths[i-1], output_uncrop_folder+'small_uncr'+path_end)\n",
    "        \n",
    "        flag_L = 'stay in loop'\n",
    "        flag_S = 'stay in loop'\n",
    "        i+=1\n",
    "        \n",
    "        # loop through all the possible sizes of the Gaussian-Blur-kernel\n",
    "        for GBlur_size in range(GBlur_bottom_value,GBlur_top_value,GBlur_interval):\n",
    "#                 print('->GBlur =', GBlur_size)\n",
    "            if flag_L == 'move to next image':\n",
    "                continue\n",
    "#                 # loop through all the possible canny thresholds\n",
    "#                 for canny_th_min, canny_th_max in zip(low_list, high_list):\n",
    "#                     if flag == 'move to next image':\n",
    "#                         continue\n",
    "\n",
    "\n",
    "            # load the image and compute the ratio of the old height\n",
    "            # to the new height, clone it, and resize it\n",
    "            #image = cv2.imread(args[\"image\"])\n",
    "            image = cv2.imread(im_path)\n",
    "            ratio = image.shape[0] /height_working_image\n",
    "            orig = image.copy()\n",
    "            image = imutils.resize(image, height = int(height_working_image))\n",
    "            \n",
    "#             cv2.imshow(\"Color\", image)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "\n",
    "            # convert the image to grayscale, blur it, and find edges\n",
    "            # in the image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (GBlur_size, GBlur_size), 0)\n",
    "            # line below was not in original script, put by Nestor\n",
    "#                 gray = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,adapThresh_size,2)\n",
    "\n",
    "            ########\n",
    "            # Calculate the canny-edged image based on the median value of image\n",
    "            v = np.median(gray)\n",
    "            sigma = 0.33\n",
    "\n",
    "            # apply automatic Canny edge detection using the computed median\n",
    "            lower = int(max(0, (1.0 - sigma) * v))\n",
    "            upper = int(min(255, (1.0 + sigma) * v))\n",
    "            edged = cv2.Canny(gray, lower, upper)\n",
    "            ########\n",
    "\n",
    "            # iterating over increasing and decreasing the width of the edges to get rid of gaps and noisy fine structure\n",
    "            edged = cv2.dilate(edged, None, iterations=1)\n",
    "            edged = cv2.erode(edged, None, iterations=1)\n",
    "\n",
    "#             cv2.imshow(\"Edged\", edged)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "            # find the contours in the edged image, keeping only the\n",
    "            # largest ones, and initialize the screen contour\n",
    "            cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cnts = imutils.grab_contours(cnts)\n",
    "            cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:10]\n",
    "\n",
    "            # loop over the contours\n",
    "            try:\n",
    "                for c in cnts:\n",
    "                    # approximate the contour\n",
    "                    peri = cv2.arcLength(c, True)\n",
    "                    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "\n",
    "                    # if our approximated contour has four points, then we\n",
    "                    # can assume that we have found our screen\n",
    "                    if len(approx) == 4:\n",
    "                        warped = four_point_transform(image, approx.reshape(4, 2)) # first version used orig, instead of image. And the there was also a ratio-factor, which I avoided\n",
    "                        path_end = os.path.basename(im_path)\n",
    "\n",
    "                        # shape is set to (500,375)\n",
    "                        h, w = float(max(warped.shape[0],warped.shape[1])), float(min(warped.shape[0],warped.shape[1]))\n",
    "                        if h/w>h_w_ratio_large_lower and h/w<h_w_ratio_large_upper and h<0.9*image.shape[0] and h>50 and w>50:\n",
    "                            # repeat the crop, but in the original image in original size and save this crop\n",
    "                            warped_large_dim = four_point_transform(orig, approx.reshape(4, 2)*ratio)\n",
    "                            cv2.imwrite(output_folder+path_end, warped_large_dim)\n",
    "                            flag_L = 'move to next image'\n",
    "                            \n",
    "        \n",
    "                            # once the big test device is found loop throught the contours again and try to find the smaller test device\n",
    "                            area_big = cv2.contourArea(approx)\n",
    "\n",
    "                            for cnt in cnts:\n",
    "                                peri = cv2.arcLength(cnt, True)\n",
    "                                # the epsilon value below is changed to 0.08 from 0.02 below, because we want it to approximate\n",
    "                                # more roughly to a quader. Look up the docs to cv2.approxPolyDP\n",
    "                                approx_s = cv2.approxPolyDP(cnt, 0.08 * peri, True)\n",
    "                                area_small = cv2.contourArea(approx_s)\n",
    "#                                     if len(approx_s)==4 and area_small>0.85*area_big/1.46:# and area_small<1.15*factor*area_big and width_calc(approx_s)>0.85*np.sqrt(factor)*w and width_calc(approx_s)<1.15*np.sqrt(factor)*w:\n",
    "#                                         print('SMALL LOOP2')\n",
    "                                if len(approx_s)==4 and area_small>lower_area_boundary*area_ratio*area_big and area_small<top_area_boundary*area_ratio*area_big and width_calc(approx_s)>lower_area_boundary*np.sqrt(area_ratio)*w and width_calc(approx_s)<top_area_boundary*np.sqrt(area_ratio)*w:\n",
    "\n",
    "#                                     cv2.drawContours(image, [approx_s], -1, (0, 255, 0), 2)\n",
    "#                                     cv2.imshow(\"Outline\", image)\n",
    "#                                     cv2.waitKey(0)\n",
    "                                    warped_s = four_point_transform(orig, approx_s.reshape(4, 2)*ratio)\n",
    "                                    cv2.imwrite(output_folder+'small_'+path_end, warped_s)\n",
    "                                    flag_S = 'move to next image' \n",
    "                            break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import imutils\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correct_orientation_RDTs(input_folder, output_folder, file_extensions, blur_size,\n",
    "                            L_top, L_bottom, L_left, L_right,\n",
    "                            S_top, S_bottom, S_left, S_right):\n",
    "    '''Function that corerctly orients all the images in the given folder to be top up. Function calculates the brightness of the ends \n",
    "    of the RDTs and based on that makes a decision where is top and down. First, function for portrait turning needs to be called.\n",
    "    L_top, S_top a.s.o provide the relative margins of the cropped areas of interest (AIO)'''\n",
    "    \n",
    "    file_names = [fn for fn in sorted(os.listdir(input_folder))\n",
    "                  if any(fn.endswith(ext) for ext in file_extensions)]\n",
    "    paths = [input_folder+file_name for file_name in file_names]\n",
    "    \n",
    "    for im_path in paths[:]:\n",
    "        im = cv2.imread(im_path)\n",
    "        # rotate the images, if the second dimension is larger\n",
    "        if im.shape[1]>im.shape[0]:\n",
    "            im = cv2.rotate(im, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        # select only the blue channel, there is the greatrst contrast between the red blood dot and the other end of the stripe\n",
    "        im_blue = im[:,:,0]\n",
    "        # blur the image slightly to get rid of outlying dark and bright pixels compromising the calculations.\n",
    "        # Uses blur_size for the size of blurred kernels\n",
    "        im_blue_blur = cv2.GaussianBlur(im_blue, (blur_size, blur_size), 0)\n",
    "        # get the height (=500) and width of the image \n",
    "        h, w = im_blue_blur.shape[0], im_blue_blur.shape[1]\n",
    "        # split the image into 2 parts, centered around the possible location of the red blood dot\n",
    "        im_part1 = im_blue_blur[int(0.1*h):int(0.3*h),int(0.3*w):-int(0.3*w)]\n",
    "        im_part2 = im_blue_blur[-int(0.3*h):-int(0.1*h),int(0.3*w):-int(0.3*w)]\n",
    "        \n",
    "        # check if the second part of the image has a larger gradient (the red spot) and rotate the image if so\n",
    "        if (np.max(im_part1)-np.min(im_part1))/np.mean(im_part1) > (np.max(im_part2)-np.min(im_part2))/np.mean(im_part2):\n",
    "            im = cv2.rotate(im, cv2.ROTATE_180)\n",
    "        \n",
    "        path_end = os.path.basename(im_path)\n",
    "        \n",
    "        # here the function differentiates between the images of different devices,\n",
    "        # becasue these different devices would have different dimensions, that are referenced for cropping\n",
    "        # In the lines below we differentiate between 2 different devices, one 'small' and one that isn't, but one could add more if-clauses for differentiations\n",
    "        # L_top, S_top a.s.o provide the relative margins of the cropped areas of interest (AIO)\n",
    "        if 'small' in im_path:\n",
    "            im_AOI = im[int(S_top*h):int(S_bottom*h), int(S_left*w):int(S_right*w)]\n",
    "            cv2.imwrite(output_folder+'AOI'+path_end, im_AOI)\n",
    "        else:\n",
    "            im_AOI = im[int(L_top*h):int(L_bottom*h), int(L_left*w):int(L_right*w)]\n",
    "            cv2.imwrite(output_folder+'AOI'+path_end, im_AOI)\n",
    "        \n",
    "#         cv2.imshow(\"Edged1\", im_AOI)\n",
    "#         cv2.imshow(\"Edged2\", im_part2)\n",
    "#         cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 0\n",
      "loop: 1\n",
      "loop: 2\n",
      "loop: 3\n",
      "loop: 4\n",
      "loop: 5\n",
      "loop: 6\n",
      "loop: 7\n",
      "loop: 8\n",
      "loop: 9\n",
      "loop: 10\n",
      "loop: 11\n",
      "loop: 12\n",
      "loop: 13\n",
      "loop: 14\n",
      "loop: 15\n",
      "loop: 16\n",
      "loop: 17\n",
      "loop: 18\n",
      "loop: 19\n",
      "loop: 20\n",
      "loop: 21\n",
      "loop: 22\n",
      "loop: 23\n",
      "loop: 24\n",
      "loop: 25\n",
      "loop: 26\n",
      "loop: 27\n",
      "loop: 28\n",
      "loop: 29\n",
      "loop: 30\n",
      "loop: 31\n",
      "loop: 32\n",
      "loop: 33\n",
      "loop: 34\n",
      "loop: 35\n",
      "loop: 36\n",
      "loop: 37\n",
      "loop: 38\n",
      "loop: 39\n",
      "loop: 40\n",
      "loop: 41\n",
      "loop: 42\n",
      "loop: 43\n",
      "loop: 44\n",
      "loop: 45\n",
      "loop: 46\n",
      "loop: 47\n",
      "loop: 48\n",
      "loop: 49\n",
      "loop: 50\n",
      "loop: 51\n",
      "loop: 52\n",
      "loop: 53\n",
      "loop: 54\n",
      "loop: 55\n",
      "loop: 56\n",
      "loop: 57\n",
      "loop: 58\n",
      "loop: 59\n",
      "loop: 60\n",
      "loop: 61\n",
      "loop: 62\n",
      "loop: 63\n",
      "loop: 64\n",
      "loop: 65\n",
      "loop: 66\n",
      "loop: 67\n",
      "loop: 68\n",
      "loop: 69\n",
      "loop: 70\n",
      "loop: 71\n",
      "loop: 72\n",
      "loop: 73\n",
      "loop: 74\n",
      "loop: 75\n",
      "loop: 76\n",
      "loop: 77\n",
      "loop: 78\n",
      "loop: 79\n",
      "loop: 80\n",
      "loop: 81\n",
      "loop: 82\n",
      "loop: 83\n",
      "loop: 84\n",
      "loop: 85\n",
      "loop: 86\n",
      "loop: 87\n",
      "loop: 88\n",
      "loop: 89\n",
      "loop: 90\n",
      "loop: 91\n",
      "loop: 92\n",
      "loop: 93\n",
      "loop: 94\n",
      "loop: 95\n",
      "loop: 96\n",
      "loop: 97\n",
      "loop: 98\n",
      "loop: 99\n"
     ]
    }
   ],
   "source": [
    "main(input_images_folder, output_cropped_folder, output_uncropped_folder,\n",
    "     ['.jpg', '.png'],\n",
    "    height_working_image=height_working_image, area_ratio=1/1.46, h_w_ratio_large_lower=2.6, h_w_ratio_large_upper=3.5)\n",
    "\n",
    "correct_orientation_RDTs(output_cropped_folder, output_AOI_folder, \n",
    "                         ['.png', '.jpg'], blur_size=blur_size_correct_orientation, \n",
    "                         L_top=L_top, L_bottom=L_bottom, L_left=L_left, L_right=L_right,\n",
    "                         S_top=S_top, S_bottom=S_bottom, S_left=S_left, S_right=S_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks regarding the performance of the code:\n",
    "The code doesn't find the contours of the paper-square if:\n",
    "- there is a grass going over square\n",
    "- a shadow is being thrown by the deice itself on the paper-square\n",
    "    - does reducing the blur help? Or maybe increasing it??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pictures of each step in the program\n",
    "- check how saving the coordinates from the pictures might be possible\n",
    "\n",
    "\n",
    "- ask for actual test pictures\n",
    "- adjust the contrast so that it can crop around our aoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd idea: try crosssection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next things to do:\n",
    "- Instead of ratio height/width do by pixel size\n",
    "- Use the threshold function again, maybe\n",
    "- Try to reach 80% accuracy\n",
    "- afterwards, check if images have correct orientation (e.g. by checking what is the brightness ratio between the two sides of the image)\n",
    "- take a cross section through the image and check if cross section fulfills a filter criterium\n",
    "- crop around area of interest. Just hard code the geometrical coordinates of the stripes section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New idea:\n",
    "- instead of detecting the big square, detect the area of interest where the stripes are\n",
    "- requires to male adjustments to the thresholding and canny\n",
    "- else, should be easy to implement, just need to filter by pixel size and ratio\n",
    "\n",
    "\n",
    "- show Val both options:\n",
    "        - dilate applied on the whole platter\n",
    "        - dilate applied only on the big test\n",
    "        - result, 05.02.: 93/100 images cropped, 81/93 are correct test, 1/93 is complete black\n",
    "        - with blur of (7,7) the correct tests increases to 89/93\n",
    "        - blur (9,9): 90/92\n",
    "        - best performance: progressive blur-kernel size, starting at 9, no threshold: 99/100 cropped, 96/99 correct test, 1/99 is completely black\n",
    "        - further tinkering: change dilate iterations, change threshold to Gaussian instead of adaptive\n",
    "        \n",
    "- 06.02: with factoring between 07-1.3 works optimal, for 0.6-1.4 there are false positiivs coming from the big test device. Check in the images to see why some are being cropped correct, what is wrong with their particular contours\n",
    "- 08.02.: The circle detection is not a great criterium for orientating the image, because sometimes the image is poorly cropped, the cirle is distorted and not properly detected by the algoeithm. Furthermore it would only work in the small device case, for the larger device the opening is not really round - blurr helps for the detection of a circle in the smaller device type, canny gives sometimes errors.\n",
    "    - check if there would be better cropping if the image would be resized to length 1000 instead of 500 - yes, it is.\n",
    "    - maybe ignore all images where the circle can not be detected. This should reduce the pool by further 10-20%\n",
    "    - instead of detecting circle count the number of contours in the top 25% of the image and in the bottom 25% of the image. Count them and where they are =3, this must be the top (with the label \"HIV\")\n",
    "    \n",
    "- increase the height of all images from 500 to 1000. Why is this making the small devices less detectable?\n",
    "\n",
    "- 15.02.: Orientation fails for one of the pictures. Check why - problem fixed\n",
    "\n",
    "Next:\n",
    "- +crop out all areas of interest\n",
    "- +make all the hardcoded imperical variables input that you can choose\n",
    "- +sort the issur with pictures that are not cropped not being saved in the designated folder\n",
    "\n",
    "Next:\n",
    "- prevent small device to be saved twice\n",
    "- implement initial cropping by the 4 small squares, not by the shapes of the devices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv35)",
   "language": "python",
   "name": "cv_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

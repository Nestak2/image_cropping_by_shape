{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for image cropping and perspective transform\n",
    "Taken from the developer Adrian Rosebrock and used as main source for OpenNoteScanner:\n",
    "\n",
    "https://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/\n",
    "\n",
    "https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch.transform import four_point_transform\n",
    "#from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import imutils\n",
    "from shutil import copyfile\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User selected input values\n",
    "The first part needs user change, the second part can probably be kept with the default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### the following features need to be changed by the user to desired values\n",
    "\n",
    "# file path to the input images, that will be cropped by the script\n",
    "input_images_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/Val_AHRI_Test_images/'\n",
    "# output folder where the intermediate cropped images will be stored\n",
    "output_cropped_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/cropped_NEW/'\n",
    "# output folder where the images will be stored in which the squares could not be successfully detected\n",
    "output_uncropped_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/uncropped/'\n",
    "# output folder where eventually the defined areas of interest from the images will be stored\n",
    "output_AOI_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/AOI/'\n",
    "\n",
    "# define the coordinates of your areas of interest, as height and width ratio in respect to the 4 squares.\n",
    "# The coordinates will have the form of: \n",
    "# image_AOI = im[int(x0*height_im):int(x1*height_im), int(y0*width_im):int(y1*width_im)]\n",
    "x0, x1 = 0.09, 0.89\n",
    "y0, y1 = 0.56, 0.90\n",
    "\n",
    "####### the features below this line can be probably kept with the given default values,\n",
    "####### but the user may consider changing them if script performance is bad\n",
    "\n",
    "# what type of image files do you want the script to opearate on\n",
    "file_extensions = ['.jpg', '.png']\n",
    "\n",
    "# the size of the kernel when threshholding is being performed\n",
    "thresh_size = 15\n",
    "\n",
    "# the height of the image will be scaled down to an image with this height, so that working can go faster \n",
    "height_working_image=1000.\n",
    "\n",
    "# defining the range of blurness in which the image will be looped for the detection of shapes\n",
    "GBlur_bottom_value=7\n",
    "GBlur_top_value=21\n",
    "GBlur_interval=2\n",
    "\n",
    "# tolerance range of what to consider a square, defining the minimal and maxim al ratio between the width and height of the object\n",
    "hw_ratio_min = 0.85\n",
    "hw_ratio_max = 1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions below are taken from Adrian Rosenbrock:\n",
    "https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "\t# initialzie a list of coordinates that will be ordered\n",
    "\t# such that the first entry in the list is the top-left,\n",
    "\t# the second entry is the top-right, the third is the\n",
    "\t# bottom-right, and the fourth is the bottom-left\n",
    "\trect = np.zeros((4, 2), dtype = \"float32\")\n",
    " \n",
    "\t# the top-left point will have the smallest sum, whereas\n",
    "\t# the bottom-right point will have the largest sum\n",
    "\ts = pts.sum(axis = 1)\n",
    "\trect[0] = pts[np.argmin(s)]\n",
    "\trect[2] = pts[np.argmax(s)]\n",
    " \n",
    "\t# now, compute the difference between the points, the\n",
    "\t# top-right point will have the smallest difference,\n",
    "\t# whereas the bottom-left will have the largest difference\n",
    "\tdiff = np.diff(pts, axis = 1)\n",
    "\trect[1] = pts[np.argmin(diff)]\n",
    "\trect[3] = pts[np.argmax(diff)]\n",
    " \n",
    "\t# return the ordered coordinates\n",
    "\treturn rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "\t# obtain a consistent order of the points and unpack them\n",
    "\t# individually\n",
    "\trect = order_points(pts)\n",
    "\t(tl, tr, br, bl) = rect\n",
    " \n",
    "\t# compute the width of the new image, which will be the\n",
    "\t# maximum distance between bottom-right and bottom-left\n",
    "\t# x-coordiates or the top-right and top-left x-coordinates\n",
    "\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "\tmaxWidth = max(int(widthA), int(widthB))\n",
    " \n",
    "\t# compute the height of the new image, which will be the\n",
    "\t# maximum distance between the top-right and bottom-right\n",
    "\t# y-coordinates or the top-left and bottom-left y-coordinates\n",
    "\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "\tmaxHeight = max(int(heightA), int(heightB))\n",
    " \n",
    "\t# now that we have the dimensions of the new image, construct\n",
    "\t# the set of destination points to obtain a \"birds eye view\",\n",
    "\t# (i.e. top-down view) of the image, again specifying points\n",
    "\t# in the top-left, top-right, bottom-right, and bottom-left\n",
    "\t# order\n",
    "\tdst = np.array([\n",
    "\t\t[0, 0],\n",
    "\t\t[maxWidth - 1, 0],\n",
    "\t\t[maxWidth - 1, maxHeight - 1],\n",
    "\t\t[0, maxHeight - 1]], dtype = \"float32\")\n",
    " \n",
    "\t# compute the perspective transform matrix and then apply it\n",
    "\tM = cv2.getPerspectiveTransform(rect, dst)\n",
    "\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    " \n",
    "\t# return the warped image\n",
    "\treturn warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def width_calc(quader):\n",
    "    '''Function that assumes quader is rectanlge and returns its width'''\n",
    "    quad = np.resize(quader, (4,2))\n",
    "    distances = []\n",
    "    for coo in quad[1:]:\n",
    "        d = np.sqrt((quad[0][0]-coo[0])**2 + (quad[0][1]-coo[1])**2)\n",
    "        distances.append(d)\n",
    "\n",
    "    return(min(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating here a main function that finds the 4 squares in the corners and uses them. Problem with this technique:\n",
    "- a fewer number of detected large test, because sometimes squares are outside of the image (~5% of images)\n",
    "- the cropped part around the smaller device are almost unusable, because it is not fixed in the slot, but moves around\n",
    "    - the problem here can be avoided by second iteration of square detection inside the slot of the small device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_around_squares(input_images_folder, output_cropped_folder, output_uncropped_folder, file_extensions, \n",
    "         height_working_image, \n",
    "         GBlur_bottom_value, GBlur_top_value, GBlur_interval):\n",
    "    \"\"\"Main function, contains the loop and all the sub-functions in it.\n",
    "    DON'T forget to add an extra '/' at the end of input and output folder.\n",
    "    area_ratio is the ratio (imaged area small device)/(imaged area large device).\n",
    "    l_w_ratio_large_upper and l_w_ratio_large_lower are the range margins for the ratio l/w between the lenght and width of the imaged large device.\"\"\"\n",
    "    \n",
    "    # create a list of all the image-paths in a given folder\n",
    "    file_names = [fn for fn in sorted(os.listdir(input_images_folder))\n",
    "                  if any(fn.endswith(ext) for ext in file_extensions)]\n",
    "    paths = [input_images_folder+file_name for file_name in file_names]\n",
    "    \n",
    "    flag_L = 'stay in loop'\n",
    "    flag_S = 'stay in loop'\n",
    "    i = 0\n",
    "                \n",
    "    # loop through all of the image-paths\n",
    "    for im_path in paths[:]:\n",
    "        path_end = os.path.basename(im_path)\n",
    "        print('loop:', i)\n",
    "        \n",
    "        flag_L = 'stay in loop'\n",
    "        flag_S = 'stay in loop'\n",
    "        i+=1\n",
    "        \n",
    "        # loop through all the possible sizes of the Gaussian-Blur-kernel\n",
    "        for GBlur_size in range(GBlur_bottom_value, GBlur_top_value, GBlur_interval):\n",
    "            if flag_L == 'move to next image':\n",
    "                continue\n",
    "\n",
    "\n",
    "            # load the image and compute the ratio of the old height\n",
    "            # to the new height, clone it, and resize it\n",
    "            #image = cv2.imread(args[\"image\"])\n",
    "            image = cv2.imread(im_path)\n",
    "            ratio = image.shape[0] /height_working_image\n",
    "            orig = image.copy()\n",
    "            image = imutils.resize(image, height = int(height_working_image))\n",
    "\n",
    "\n",
    "            # convert the image to grayscale, blur it, and find edges\n",
    "            # in the image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (GBlur_size, GBlur_size), 0)\n",
    "            # line below does threshholding in black and white\n",
    "            gray = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, thresh_size,2)\n",
    "\n",
    "            ########\n",
    "            # Calculate the canny-edged image based on the median value of image\n",
    "            v = np.median(gray)\n",
    "            sigma = 0.33\n",
    "\n",
    "            # apply automatic Canny edge detection using the computed median\n",
    "            lower = int(max(0, (1.0 - sigma) * v))\n",
    "            upper = int(min(255, (1.0 + sigma) * v))\n",
    "            edged = cv2.Canny(gray, lower, upper)\n",
    "            ########\n",
    "\n",
    "            # iterating over increasing and decreasing the width of the edges to get rid of gaps and noisy fine structure\n",
    "            edged = cv2.dilate(edged, None, iterations=1)\n",
    "            edged = cv2.erode(edged, None, iterations=1)\n",
    "\n",
    "#             cv2.imshow(\"Edged\", edged)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "            # find the contours in the edged image, keeping only the\n",
    "            # largest ones, and initialize the screen contour\n",
    "            cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cnts = imutils.grab_contours(cnts)\n",
    "            cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:30]\n",
    "            \n",
    "#             cv2.drawContours(image, cnts, -1, (0, 255, 0), 2)\n",
    "#             cv2.imshow(\"All edges\", image)\n",
    "#             cv2.waitKey(0)\n",
    "              \n",
    "            squares = []\n",
    "            square_centers = []\n",
    "            features = []\n",
    "            for c in cnts:\n",
    "                # approximate the contour\n",
    "                peri = cv2.arcLength(c, True)\n",
    "                approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "                \n",
    "#                 cv2.drawContours(image, [approx], -1, (0, 255, 0), 2)\n",
    "#                 cv2.imshow(\"Outline\", image)\n",
    "#                 cv2.waitKey(0)\n",
    "\n",
    "                # if our approximated contour has four points, then we\n",
    "                # can assume that we have found our screen\n",
    "                if len(approx) == 4:\n",
    "                    \n",
    "                    (x, y, w, h) = cv2.boundingRect(approx)\n",
    "                    ar = w / float(h)\n",
    "                    \n",
    "                    # we need first to order the points so that the first point is always the top left\n",
    "                    approx = order_points(approx.reshape((4,2))).astype(int)\n",
    "        \n",
    "                    # define the four corner points of one pf the possible squares\n",
    "                    A, B, C, D = approx[0], approx[1], approx[2], approx[3]        \n",
    "                    x, y = A[0], A[1]\n",
    "                    h, w = np.sqrt((B[1]-A[1])**2 + (B[0]-A[0])**2), np.sqrt((B[1]-C[1])**2 + (B[0]-C[0])**2)\n",
    "                    hw_ratio = w / float(h)\n",
    "\n",
    "                    # a square will have an aspect ratio that is approximately\n",
    "                    # equal to one, otherwise, the shape is a rectangle.\n",
    "                    # we also make sure that the rectangle is small\n",
    "                    if hw_ratio >= hw_ratio_min and hw_ratio <= hw_ratio_max:\n",
    "                        features.append([approx, ar, x, y, w, h])\n",
    "                        \n",
    "            # we are getting rid of outliers here\n",
    "            features = np.array(features)\n",
    "            for (approx, ar, x, y, w, h) in features:\n",
    "                if (abs(w - np.median(features[:,4])) < 0.6*np.median(features[:,4])): # 1. * np.std(features[:,4])\n",
    "                    \n",
    "                        \n",
    "                    # append the sauqres and centers of the squares into a list\n",
    "                    square_centers.append([int(x+w/2), int(y+h/2)])\n",
    "                    squares.append(approx)\n",
    "            \n",
    "            # crop the large image correspondingly,\n",
    "            # but first check if 4 squares are detected af their centers also form a square\n",
    "            \n",
    "#             print(\"squares:\", squares)\n",
    "#             print('square centers:', square_centers)\n",
    "            \n",
    "#             cv2.drawContours(image, np.array(preliminary_squares), -1, (0, 255, 0), 2)\n",
    "#             cv2.imshow(\"Preliminary Squares\", image)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "            try:\n",
    "                # get rid of overlapping square centers\n",
    "                square_centers_filtered = [square_centers[0]]\n",
    "                for sq in square_centers:\n",
    "                    flag = 'unique'\n",
    "                    for squ in square_centers_filtered:\n",
    "                        if np.sqrt((sq[0]-squ[0])**2 + (sq[1]-squ[1])**2) < 50:\n",
    "                            flag = 'not unique'\n",
    "                    if flag == 'unique':\n",
    "                        square_centers_filtered.append(sq)\n",
    "\n",
    "#                 print(\"square centers filtered:\", square_centers_filtered)\n",
    "            \n",
    "                warped = four_point_transform(orig, np.array(square_centers_filtered).reshape(4, 2)*ratio)\n",
    "                cv2.imwrite(output_cropped_folder+'NEW_'+path_end, warped)\n",
    "                flag_L = 'move to next image'\n",
    "            except:\n",
    "                if GBlur_size == (GBlur_top_value-GBlur_interval):\n",
    "                    copyfile(im_path, output_uncropped_folder+'NEW_'+path_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_around_AOI(output_cropped_folder, output_AOI_folder, file_extensions):\n",
    "    '''This function serves just to take the pictures, that have been cropped accordingly to the squares detected in them\n",
    "    and then out of these to crop out the area of interest (AOI) specified by coordinates - these are the final images'''\n",
    "    \n",
    "    file_names = [fn for fn in sorted(os.listdir(output_cropped_folder))\n",
    "                  if any(fn.endswith(ext) for ext in file_extensions)]\n",
    "    paths = [output_cropped_folder+file_name for file_name in file_names]\n",
    "    \n",
    "    for im_path in paths[:]:\n",
    "        path_end = os.path.basename(im_path)\n",
    "        \n",
    "        im = cv2.imread(im_path)\n",
    "        h, w = im.shape[0], im.shape[1]\n",
    "        # rotate the images, if the second dimension is larger\n",
    "        if im.shape[1]>im.shape[0]:\n",
    "            im = cv2.rotate(im, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        im_AOI = im[int(x0*h):int(x1*h), int(y0*w):int(y1*w)]\n",
    "        cv2.imwrite(output_AOI_folder+'AOI'+path_end, im_AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 0\n",
      "loop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/narsenov/anaconda3/envs/cv_py35/lib/python3.5/site-packages/ipykernel_launcher.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 2\n",
      "loop: 3\n",
      "loop: 4\n",
      "loop: 5\n",
      "loop: 6\n",
      "loop: 7\n",
      "loop: 8\n",
      "loop: 9\n",
      "loop: 10\n",
      "loop: 11\n",
      "loop: 12\n",
      "loop: 13\n",
      "loop: 14\n",
      "loop: 15\n",
      "loop: 16\n",
      "loop: 17\n",
      "loop: 18\n",
      "loop: 19\n",
      "loop: 20\n",
      "loop: 21\n",
      "loop: 22\n",
      "loop: 23\n",
      "loop: 24\n",
      "loop: 25\n",
      "loop: 26\n",
      "loop: 27\n",
      "loop: 28\n",
      "loop: 29\n",
      "loop: 30\n",
      "loop: 31\n",
      "loop: 32\n",
      "loop: 33\n",
      "loop: 34\n",
      "loop: 35\n",
      "loop: 36\n",
      "loop: 37\n",
      "loop: 38\n",
      "loop: 39\n",
      "loop: 40\n",
      "loop: 41\n",
      "loop: 42\n",
      "loop: 43\n",
      "loop: 44\n",
      "loop: 45\n",
      "loop: 46\n",
      "loop: 47\n",
      "loop: 48\n",
      "loop: 49\n",
      "loop: 50\n",
      "loop: 51\n",
      "loop: 52\n",
      "loop: 53\n",
      "loop: 54\n",
      "loop: 55\n",
      "loop: 56\n",
      "loop: 57\n",
      "loop: 58\n",
      "loop: 59\n",
      "loop: 60\n",
      "loop: 61\n",
      "loop: 62\n",
      "loop: 63\n",
      "loop: 64\n",
      "loop: 65\n",
      "loop: 66\n",
      "loop: 67\n",
      "loop: 68\n",
      "loop: 69\n",
      "loop: 70\n",
      "loop: 71\n",
      "loop: 72\n",
      "loop: 73\n",
      "loop: 74\n",
      "loop: 75\n",
      "loop: 76\n",
      "loop: 77\n",
      "loop: 78\n",
      "loop: 79\n",
      "loop: 80\n",
      "loop: 81\n",
      "loop: 82\n",
      "loop: 83\n",
      "loop: 84\n",
      "loop: 85\n",
      "loop: 86\n",
      "loop: 87\n",
      "loop: 88\n",
      "loop: 89\n",
      "loop: 90\n",
      "loop: 91\n",
      "loop: 92\n",
      "loop: 93\n",
      "loop: 94\n",
      "loop: 95\n",
      "loop: 96\n",
      "loop: 97\n",
      "loop: 98\n",
      "loop: 99\n"
     ]
    }
   ],
   "source": [
    "crop_around_squares(input_images_folder, output_cropped_folder, output_uncropped_folder,\n",
    "     file_extensions, height_working_image,\n",
    "     GBlur_bottom_value, GBlur_top_value, GBlur_interval)   \n",
    "\n",
    "crop_around_AOI(output_cropped_folder, output_AOI_folder, file_extensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to finish the code:\n",
    "- provide as input the coordinates of the object you are interested in\n",
    "- ask Jobie if he is the background author? Ask for permission to use the background\n",
    "- upload the background also on github\n",
    "\n",
    "## Publish 3 different codes:\n",
    "- Detecting just the largest quader shape\n",
    "- detecting a shape with given side ratio and approximate area ratio\n",
    "- detecting the 4 squares in the background and than cropping around a specified area"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv35)",
   "language": "python",
   "name": "cv_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

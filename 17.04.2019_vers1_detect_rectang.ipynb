{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for image cropping and perspective transform\n",
    "The code looks for 2 rectangular shapes in the image (see figure below), cropes them and eventually crops from them a specified area of interest (see below). Ratio of sizes of the 2 devices need to be provided. Can be easy adjusted to also work with just one device (or another number of devices).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1|2|3|4|5\n",
    "-|-|-|-|- \n",
    "<img src=\"20190417_123245.jpg\" alt=\"Drawing\" width=\"920\"/> | <img src=\"small_20190417_123245.jpg\" alt=\"Drawing\" width=\"150\"/> | <img src=\"20190417_123245_a.jpg\" alt=\"Drawing\" width=\"150\"/> | <img src=\"AOIsmall_20190417_123245.jpg\" alt=\"Drawing\" width=\"70\"/>| <img src=\"AOI20190417_123245.jpg\" alt=\"Drawing\" width=\"80\"/>\n",
    "*Image 1, the input in which the code will look for rectangular shapes with features fitting the white ticket*|*Image 2a, the small ticket cropped out*|*Image 2b, the large ticket cropped out*|*Image 3a, the area of interest. Cropped out by user provided coordinates of image 2a*|*Image 3b, the area of interest. Cropped out by user provided coordinates of image 2b*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of work of the script. In yellow frame are the 2 devices, in blue are the areas of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch.transform import four_point_transform\n",
    "#from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import imutils\n",
    "from shutil import copyfile\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User selected input values\n",
    "The first part needs user change, the second part can probably be kept with the default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### the following features need to be changed by the user to desired values\n",
    "\n",
    "# file path to the input images, that will be cropped by the script\n",
    "input_images_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/for github/test_input/'\n",
    "# output folder where the intermediate cropped images will be stored\n",
    "output_cropped_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/for github/test_output/'\n",
    "# output folder where the images will be stored in which the squares could not be successfully detected\n",
    "output_uncropped_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/for github/test_not_cropped/'\n",
    "# output folder where eventually the defined areas of interest from the images will be stored\n",
    "output_AOI_folder = '/Users/narsenov/Documents/synced_docs/i-sense/code_cropping_script/for github/test_AOI/'\n",
    "\n",
    "# define the coordinates of your areas of interest, as height and width ratio in respect to the 4 squares.\n",
    "# The coordinates will have the form of: \n",
    "# image_AOI = im[int(x0*height_im):int(x1*height_im), int(y0*width_im):int(y1*width_im)]\n",
    "# In this script 2 devixes are being detected, L is the large and S is the small\n",
    "\n",
    "# for 2018\n",
    "L_top, L_bottom = 0.45, 0.52 #0.40, 0.63 # 0.41, 0.64 # for 2017\n",
    "L_left, L_right = 0.33, 0.8 #0.37, 0.67\n",
    "S_top, S_bottom = 0.69, 0.75 #0.28, 0.63\n",
    "S_left, S_right = 0.4, 0.77 #0.32, 0.66\n",
    "\n",
    "####### the features below this line can be probably kept with the given default values,\n",
    "####### but the user may consider changing them if script performance is bad\n",
    "\n",
    "# what type of image files do you want the script to opearate on\n",
    "file_extensions = ['.jpg', '.png']\n",
    "\n",
    "# # the size of the kernel when threshholding is being performed\n",
    "# thresh_size = 15\n",
    "\n",
    "# the height of the image will be scaled down to an image with this height, so that working can go faster \n",
    "height_working_image=500.\n",
    "\n",
    "# defining the range of blurness in which the image will be looped for the detection of shapes\n",
    "GBlur_bottom_value=7\n",
    "GBlur_top_value=21\n",
    "GBlur_interval=2\n",
    "\n",
    "# are_ratio determines the approximate ratio (pixel-area small device)/(pixel-area large device)\n",
    "# the lower and top biundaries determine the tolerance range around the area_ratio, e.g.\n",
    "# for a object to be considered small device it has to fulfill \n",
    "# area_small>lower_area_boundary*area_ratio*area_big and area_small<top_area_boundary*area_ratio*area_big\n",
    "area_ratio= 1.1#1/1.46\n",
    "lower_area_boundary, top_area_boundary = 0.7, 1.3\n",
    "\n",
    "#define the range of tolerance for the ratio of h/w for the size of the large device\n",
    "h_w_ratio_large_lower= 2.0#2.6\n",
    "h_w_ratio_large_upper= 3.0#6.5\n",
    "\n",
    "# the size of the blurring kernel for the function for correct orientation\n",
    "blur_size_correct_orientation = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_points(pts):\n",
    "\t# initialzie a list of coordinates that will be ordered\n",
    "\t# such that the first entry in the list is the top-left,\n",
    "\t# the second entry is the top-right, the third is the\n",
    "\t# bottom-right, and the fourth is the bottom-left\n",
    "\trect = np.zeros((4, 2), dtype = \"float32\")\n",
    " \n",
    "\t# the top-left point will have the smallest sum, whereas\n",
    "\t# the bottom-right point will have the largest sum\n",
    "\ts = pts.sum(axis = 1)\n",
    "\trect[0] = pts[np.argmin(s)]\n",
    "\trect[2] = pts[np.argmax(s)]\n",
    " \n",
    "\t# now, compute the difference between the points, the\n",
    "\t# top-right point will have the smallest difference,\n",
    "\t# whereas the bottom-left will have the largest difference\n",
    "\tdiff = np.diff(pts, axis = 1)\n",
    "\trect[1] = pts[np.argmin(diff)]\n",
    "\trect[3] = pts[np.argmax(diff)]\n",
    " \n",
    "\t# return the ordered coordinates\n",
    "\treturn rect\n",
    "\n",
    "def four_point_transform(image, pts):\n",
    "\t# obtain a consistent order of the points and unpack them\n",
    "\t# individually\n",
    "\trect = order_points(pts)\n",
    "\t(tl, tr, br, bl) = rect\n",
    " \n",
    "\t# compute the width of the new image, which will be the\n",
    "\t# maximum distance between bottom-right and bottom-left\n",
    "\t# x-coordiates or the top-right and top-left x-coordinates\n",
    "\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "\tmaxWidth = max(int(widthA), int(widthB))\n",
    " \n",
    "\t# compute the height of the new image, which will be the\n",
    "\t# maximum distance between the top-right and bottom-right\n",
    "\t# y-coordinates or the top-left and bottom-left y-coordinates\n",
    "\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "\tmaxHeight = max(int(heightA), int(heightB))\n",
    " \n",
    "\t# now that we have the dimensions of the new image, construct\n",
    "\t# the set of destination points to obtain a \"birds eye view\",\n",
    "\t# (i.e. top-down view) of the image, again specifying points\n",
    "\t# in the top-left, top-right, bottom-right, and bottom-left\n",
    "\t# order\n",
    "\tdst = np.array([\n",
    "\t\t[0, 0],\n",
    "\t\t[maxWidth - 1, 0],\n",
    "\t\t[maxWidth - 1, maxHeight - 1],\n",
    "\t\t[0, maxHeight - 1]], dtype = \"float32\")\n",
    " \n",
    "\t# compute the perspective transform matrix and then apply it\n",
    "\tM = cv2.getPerspectiveTransform(rect, dst)\n",
    "\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    " \n",
    "\t# return the warped image\n",
    "\treturn warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def width_calc(quader):\n",
    "    '''Function that assumes quader is rectanlge and returns its width'''\n",
    "    quad = np.resize(quader, (4,2))\n",
    "    distances = []\n",
    "    for coo in quad[1:]:\n",
    "        d = np.sqrt((quad[0][0]-coo[0])**2 + (quad[0][1]-coo[1])**2)\n",
    "        distances.append(d)\n",
    "\n",
    "    return(min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_folder, output_folder, output_uncrop_folder, file_extensions, \n",
    "         height_working_image):\n",
    "    \"\"\"Main function, contains the loop and all the sub-functions in it.\n",
    "    DON'T forget to add an extra '/' at the end of input and output folder.\n",
    "    area_ratio is the ratio (imaged area small device)/(imaged area large device).\n",
    "    h_w_ratio_large_upper and h_w_ratio_large_lower are the range margins for the ratio h/w between the lenght and width of the imaged large device.\"\"\"\n",
    "    # create a list of all the image-paths in a given folder\n",
    "\n",
    "    file_names = [fn for fn in sorted(os.listdir(input_folder))\n",
    "                  if any(fn.endswith(ext) for ext in file_extensions)]\n",
    "    paths = [input_folder+file_name for file_name in file_names]\n",
    "    \n",
    "    flag_L = 'stay in loop'\n",
    "    flag_S = 'stay in loop'\n",
    "    i = 0\n",
    "                \n",
    "    # loop through all of the image-paths\n",
    "    for im_path in paths[:]:\n",
    "        print('loop:', i)\n",
    "        \n",
    "        if i>0 and flag_L == 'stay in loop':\n",
    "            copyfile(paths[i-1], output_uncrop_folder+'large_uncr'+path_end)\n",
    "        if i>0 and flag_S == 'stay in loop':\n",
    "            copyfile(paths[i-1], output_uncrop_folder+'small_uncr'+path_end)\n",
    "        \n",
    "        flag_L = 'stay in loop'\n",
    "        flag_S = 'stay in loop'\n",
    "        i+=1\n",
    "        \n",
    "        # loop through all the possible sizes of the Gaussian-Blur-kernel\n",
    "        for GBlur_size in range(GBlur_bottom_value,GBlur_top_value,GBlur_interval):\n",
    "            if flag_L == 'move to next image':\n",
    "                continue\n",
    "#                 # loop through all the possible canny thresholds\n",
    "#                 for canny_th_min, canny_th_max in zip(low_list, high_list):\n",
    "#                     if flag == 'move to next image':\n",
    "#                         continue\n",
    "\n",
    "\n",
    "            # load the image and compute the ratio of the old height\n",
    "            # to the new height, clone it, and resize it\n",
    "            #image = cv2.imread(args[\"image\"])\n",
    "            image = cv2.imread(im_path)\n",
    "            ratio = image.shape[0] /height_working_image\n",
    "            orig = image.copy()\n",
    "            image = imutils.resize(image, height = int(height_working_image))\n",
    "            \n",
    "#             cv2.imshow(\"Color\", image)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "\n",
    "            # convert the image to grayscale, blur it, and find edges\n",
    "            # in the image\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (GBlur_size, GBlur_size), 0)\n",
    "            # line below was not in original script, put by Nestor\n",
    "#                 gray = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,adapThresh_size,2)\n",
    "\n",
    "            ########\n",
    "            # Calculate the canny-edged image based on the median value of image\n",
    "            v = np.median(gray)\n",
    "            sigma = 0.33\n",
    "\n",
    "            # apply automatic Canny edge detection using the computed median\n",
    "            lower = int(max(0, (1.0 - sigma) * v))\n",
    "            upper = int(min(255, (1.0 + sigma) * v))\n",
    "            edged = cv2.Canny(gray, lower, upper)\n",
    "            ########\n",
    "\n",
    "            # iterating over increasing and decreasing the width of the edges to get rid of gaps and noisy fine structure\n",
    "            edged = cv2.dilate(edged, None, iterations=1)\n",
    "            edged = cv2.erode(edged, None, iterations=1)\n",
    "\n",
    "#             cv2.imshow(\"Edged\", edged)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "            # find the contours in the edged image, keeping only the\n",
    "            # largest ones, and initialize the screen contour\n",
    "            cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cnts = imutils.grab_contours(cnts)\n",
    "            cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:10]\n",
    "\n",
    "            # loop over the contours\n",
    "            try:\n",
    "                for c in cnts:\n",
    "                    # approximate the contour\n",
    "                    peri = cv2.arcLength(c, True)\n",
    "                    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "\n",
    "                    # if our approximated contour has four points, then we\n",
    "                    # can assume that we have found our screen\n",
    "                    if len(approx) == 4:\n",
    "                        warped = four_point_transform(image, approx.reshape(4, 2)) # first version used orig, instead of image. And the there was also a ratio-factor, which I avoided\n",
    "                        path_end = os.path.basename(im_path)\n",
    "\n",
    "                        # shape is set to (500,375)\n",
    "                        h, w = float(max(warped.shape[0],warped.shape[1])), float(min(warped.shape[0],warped.shape[1]))\n",
    "                        if h/w>h_w_ratio_large_lower and h/w<h_w_ratio_large_upper:# and h<0.9*image.shape[0] and (h>50 or w>50):\n",
    "                            # repeat the crop, but in the original image in original size and save this crop\n",
    "                            warped_large_dim = four_point_transform(orig, approx.reshape(4, 2)*ratio)\n",
    "                            cv2.imwrite(output_folder+path_end, warped_large_dim)\n",
    "                            flag_L = 'move to next image'\n",
    "                            \n",
    "        \n",
    "                            # once the big test device is found loop throught the contours again and try to find the smaller test device\n",
    "                            area_big = cv2.contourArea(approx)\n",
    "                            for cnt in cnts:\n",
    "                                peri = cv2.arcLength(cnt, True)\n",
    "                                # the epsilon value below is changed to 0.08 from 0.02 below, because we want it to approximate\n",
    "                                # more roughly to a quader. Look up the docs to cv2.approxPolyDP\n",
    "                                approx_s = cv2.approxPolyDP(cnt, 0.08 * peri, True)\n",
    "                                area_small = cv2.contourArea(approx_s)\n",
    "                                if len(approx_s)==4 and area_small>lower_area_boundary*area_ratio*area_big and area_small<top_area_boundary*area_ratio*area_big and width_calc(approx_s)>lower_area_boundary*np.sqrt(area_ratio)*w and width_calc(approx_s)<top_area_boundary*np.sqrt(area_ratio)*w:\n",
    "\n",
    "#                                     cv2.drawContours(image, [approx_s], -1, (0, 255, 0), 2)\n",
    "#                                     cv2.imshow(\"Outline\", image)\n",
    "#                                     cv2.waitKey(0)\n",
    "                                    warped_s = four_point_transform(orig, approx_s.reshape(4, 2)*ratio)\n",
    "                                    cv2.imwrite(output_folder+'small_'+path_end, warped_s)\n",
    "                                    flag_S = 'move to next image' \n",
    "                            break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import imutils\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correct_orientation_RDTs(input_folder, output_folder, file_extensions, blur_size,\n",
    "                            L_top, L_bottom, L_left, L_right,\n",
    "                            S_top, S_bottom, S_left, S_right):\n",
    "    '''Function that corerctly orients all the images in the given folder to be top up. Function calculates the brightness of the ends \n",
    "    of the RDTs and based on that makes a decision where is top and down. First, function for portrait turning needs to be called.\n",
    "    L_top, S_top a.s.o provide the relative margins of the cropped areas of interest (AIO)'''\n",
    "    \n",
    "    file_names = [fn for fn in sorted(os.listdir(input_folder))\n",
    "                  if any(fn.endswith(ext) for ext in file_extensions)]\n",
    "    paths = [input_folder+file_name for file_name in file_names]\n",
    "    \n",
    "    for im_path in paths[:]:\n",
    "        im = cv2.imread(im_path)\n",
    "        # rotate the images, if the second dimension is larger\n",
    "        if im.shape[1]>im.shape[0]:\n",
    "            im = cv2.rotate(im, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        # select only the blue channel, there is the greatrst contrast between the red blood dot and the other end of the stripe\n",
    "        im_blue = im[:,:,0]\n",
    "        # blur the image slightly to get rid of outlying dark and bright pixels compromising the calculations.\n",
    "        # Uses blur_size for the size of blurred kernels\n",
    "        im_blue_blur = cv2.GaussianBlur(im_blue, (blur_size, blur_size), 0)\n",
    "        # get the height (=500) and width of the image \n",
    "        h, w = im_blue_blur.shape[0], im_blue_blur.shape[1]\n",
    "        # split the image into 2 parts, centered around the possible location of the red blood dot\n",
    "        im_part1 = im_blue_blur[int(0.1*h):int(0.3*h),int(0.3*w):-int(0.3*w)]\n",
    "        im_part2 = im_blue_blur[-int(0.3*h):-int(0.1*h),int(0.3*w):-int(0.3*w)]\n",
    "        \n",
    "        # check if the second part of the image has a larger gradient (the red spot) and rotate the image if so\n",
    "#         if (np.max(im_part1)-np.min(im_part1))/np.mean(im_part1) > (np.max(im_part2)-np.min(im_part2))/np.mean(im_part2):\n",
    "#             im = cv2.rotate(im, cv2.ROTATE_180)\n",
    "        \n",
    "        path_end = os.path.basename(im_path)\n",
    "        \n",
    "        # here the function differentiates between the images of different devices,\n",
    "        # becasue these different devices would have different dimensions, that are referenced for cropping\n",
    "        # In the lines below we differentiate between 2 different devices, one 'small' and one that isn't, but one could add more if-clauses for differentiations\n",
    "        # L_top, S_top a.s.o provide the relative margins of the cropped areas of interest (AIO)\n",
    "        if 'small' in im_path:\n",
    "            im_AOI = im[int(S_top*h):int(S_bottom*h), int(S_left*w):int(S_right*w)]\n",
    "            cv2.imwrite(output_folder+'AOI'+path_end, im_AOI)\n",
    "        else:\n",
    "            im_AOI = im[int(L_top*h):int(L_bottom*h), int(L_left*w):int(L_right*w)]\n",
    "            cv2.imwrite(output_folder+'AOI'+path_end, im_AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 0\n"
     ]
    }
   ],
   "source": [
    "main(input_images_folder, output_cropped_folder, output_uncropped_folder,\n",
    "     ['.jpg', '.png'],\n",
    "    height_working_image=height_working_image)\n",
    "\n",
    "correct_orientation_RDTs(output_cropped_folder, output_AOI_folder, \n",
    "                         ['.png', '.jpg'], blur_size=blur_size_correct_orientation, \n",
    "                         L_top=L_top, L_bottom=L_bottom, L_left=L_left, L_right=L_right,\n",
    "                         S_top=S_top, S_bottom=S_bottom, S_left=S_left, S_right=S_right)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv35)",
   "language": "python",
   "name": "cv_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
